---
slug: 'deploying-an-url-shortener-on-aws'
title: 'Deploying an URL shortener on AWS'
pubDate: 2025-08-13T18:04:10.163Z
draft: true
tags:
  - cloud
  - aws
  - spring
---

## 0. Introduction

In this article, I'll detail my experience deploying a simple application on AWS. Since it was my first time really working with AWS (and cloud in general), I didn't use any IaC language, so the entire setup was done through the AWS console. I'm planning to recreate this deployment using Terraform in the future.

But before talking about my AWS experience, I'll give you an overview of the application itself.

## 1. The application

Since the application was not the focus on this project, I built a simple URL shortener and divided it in two components: backend and frontend. Its source code can be found [here](https://github.com/davisiqueira1/url-shortener).

### 1.1 Backend

The backend was built using Spring Boot. Its core functionality is to take a long URL and generate a short code, mapping it back to the long URL. To add another feature, I decided to make the short codes expire 7 days after their creation.

To implement this expiration date, I chose to use MongoDB as my database, since it has built-in TTL (time to live) indexing and can be easily integrated with Spring Boot by adding a dependency named [Spring Data MongoDB](https://spring.io/projects/spring-data-mongodb).

With that said, let's define our entity model:

```java
@Document(collection = "url")
@Setter
@Getter
@NoArgsConstructor
@AllArgsConstructor
public class UrlModel {
    @Id
    private String id;

    private String longUrl;
    private String shortCode;

    @Indexed(name = "expirationDate_ttl", expireAfter = "0s")
    private Date expirationDate;
}
```

As you can see, the model is quite straightforward: an ID managed by MongoDB, two strings used to map to each other and an expirationDate indexed by the database.

A detailed review of the API's internal logic is beyond the scope of this article. For our purposes, it's sufficient to understand its interface, just as the frontend does.

The server listens on port 8080 and exposes two endpoints:

- `/api/shorten`: given a long URL, it returns a short code mapping back to the url.
- `/{shortCode}`: given a short code, it redirects the user to the corresponding long URL.

Only one thing in this project is simpler than our backend: our frontend.

### 1.2 Frontend

The frontend was so simple that I built it using HTML, CSS, and JavaScript.

One page, one input and one button: that's all we need.

<img src="/assets/url-shortener-frontend.png" width="150"/><br><br>

## 2. Deploying it on AWS

Let's start our really long journey by defining what's our goal.

At the end of this article we'll have configured two routes of services:

1. Frontend: User -> CloudFront -> S3
2. API calls: Frontend -> ALB (Load Balancer) -> Nginx (EC2) -> Spring Boot -> MongoDB

So, let's cut to action.

### 2.1 Unexpected start: database deployment

To start our journey deploying things on AWS we'll host our MongoDB on... _(drum roll)_ ... MongoDB Atlas!!!!

I know, I know, MongoDB Atlas isn't an AWS service... I know, you can stop throwing tomatoes at me now.

That's the only regret I have on this project: I didn't plan the whole thing when I was coding the backend. As I told you in the beginning of this article, it was my first time working with AWS, so I didn't know they don't have a (N)RDS service.

I know you can install and run MongoDB on an EC2 instance, but that was not my goal. Didn't use DynamoDB too (but I'm planning to try it in the future!), so the last option was hosting it on MongoDB Atlas.

_Side note: as I was writing this, I found out that AWS offers a service that emulates MongoDB, it's called [DocumentDB](https://aws.amazon.com/pt/documentdb/)._

The step-by-step is straightforward: just create an [account](https://account.mongodb.com/account/register) and instantiate a flex cluster.

### 2.2 You can't run from linux: application deployment

And we can finally start our AWS journey.

#### 2.2.1 Lauching our EC2

To launch an EC2 instance, go to EC2 -> Instances -> Launch instance.

Instance setup:

- Name: ...
- AMI: Ubuntu Server 22.04 LTS
- Instance type: t3.micro
- Key pair: generate and save your key, we'll use it to remotely connect to our instance
- Security Group (SG): create a SG and allow the following protocols/IPs:
  - SSH (22): accept connections only from your IP.
  - HTTP (80): accept connections from all origins (0.0.0.0/0).
- Storage: 15gb / gp3

Launch and wait for it to become available. We'll connect to our EC2 remotely using SSH and the key pair generated on the setup (I hope you stored it).

To connect, simply run this command in your terminal:

```bash
ssh -i your-key.pem ubuntu@your-ec2-public-ip
```

- `-i your-key.pem`: selects a file from which the identity (private key) for public key authentication is read
- Your EC2 public IP can be found on the details tab

Now we are inside our EC2 instance!

First, we need to update our linux packages and install some dependencies:

```bash
sudo apt update && sudo apt -y install openjdk-21-jdk nginx
```

You'll probably need to reboot your system, so run this command and wait a bit before trying to connect to your EC2 again:

```bash
sudo reboot
```

Now that everything is up-to-date, it's time to get our application running.

#### 2.2.2 Creating a dedicated service and user to run our application

To increase the system security, we'll run our application in a more isolated environment. This approach involves two key components:

- A dedicated service with hardening actions.
- A dedicated user that will have limited permissions:
  - Access the application folder
  - Run the service
  - Run the .jar file

First, we need to get our application .jar file.

To generate it, go to our backend directory and run the following maven command:

```bash
mvn package
```

Our .jar file will be generated on the /target directory.

To send it to our EC2, we need to use the following `scp` [(Secure Copy Protocol)](https://www.man7.org/linux/man-pages/man1/scp.1.html) command:

```bash
scp -i your-key.pem your-jar.jar ubuntu@your-ec2-public-ip:/home/ubuntu
```

This will copy the .jar file to the home directory of the ubuntu user on your EC2 instance.

Now that we have our application file, let's create a dedicated user to run our service:

```bash
sudo useradd -s /usr/sbin/nologin userapp
```

- `-s /usr/sbin/nologin`: sets the user's shell to a non-interactive shell, preventing anyone from logging in as this user.
- `userapp`: user's name

We'll create a directory to store our .jar file and give folder ownership to our dedicated user:

```bash
sudo mkdir -p /opt/userapp &&
sudo chown userapp:userapp /opt/userapp
```

Move the .jar to our application directory and give file ownership to our dedicated user:

```bash
sudo mv /home/ubuntu/your-jar.jar /opt/userapp/app.jar &&
sudo chown userapp:userapp /opt/userapp/app.jar
```

To store environment variables safely, we'll create a dedicated file and only grant read/write permission to root:

```bash
sudo touch /etc/urlapp.env &&

sudo bash -c 'cat > /etc/urlapp.env' <<'EOF'
MONGODB_URI="mongodb+srv://user:pwd@cluster.mongodb.net/db"
PORT=8080
EOF &&

sudo chmod 600 /etc/urlapp.env
```

- `-c '...'`: will not open a shell. Instead, it'll run the following string as a command
- `chmod 600`: sets the file permissions so that only the owner (the root user, in this case) has read and write access

Now that our directories were created and the permissions were set, we'll create a systemd _unit file_ to define our service:

```bash
sudo touch /etc/systemd/system/urlapp.service
```

Our service definition will look like this:

_Side note: Iâ€™ve added comments to make the file easier to understand._

```bash
sudo bash -c 'cat > /etc/systemd/system/urlapp.service' <<'EOF'
[Unit]
Description=URL Shortener
# The service will start after the network is online.
After=network-online.target
Wants=network-online.target

[Service]
# Defines service user and group
User=userapp
Group=userapp

# Environment variables file to be loaded into the service process.
EnvironmentFile=/etc/urlapp.env

WorkingDirectory=/opt/urlapp
ExecStart=/usr/bin/java -jar /opt/urlapp/app.jar

# Treat exit code 143 (SIGTERM) as a successful, clean shutdown.
SuccessExitStatus=143

# If the process crashes, systemd will restart the service.
Restart=always
RestartSec=3

LimitNOFILE=65535

# Hardening (isolate the process from the system)

NoNewPrivileges=true
ProtectSystem=strict
ReadWritePaths=/opt/urlapp
ProtectHome=true
PrivateTmp=true
PrivateDevices=true
ProtectHostname=true
ProtectClock=true
ProtectKernelTunables=true
ProtectKernelModules=true
ProtectControlGroups=true
LockPersonality=true

# Tends to break Java projects because the JVM cannot allocate executable memory.
# Error example: Failed to mark memory page as executable - check if grsecurity/PaX is enabled
# MemoryDenyWriteExecute=true

[Install]
WantedBy=multi-user.target

EOF
```

Now, let's tell systemd about our new service and get it running:

```bash
sudo systemctl daemon-reload &&
sudo systemctl enable urlapp &&
sudo systemctl start urlapp
```

To check our service status, run:

```bash
sudo systemctl status urlapp
```

At this point, everything should be running smoothly, without problems. If the service is inactive (or has failed), we can check our service logs with the following command:

```bash
journalctl -u urlapp -f
```

- `-f`: "tails" the log, showing new lines in real-time

#### 2.2.3 Nginx setup

If you're paying attention to what we're doing, you might be asking yourself a question: "Okay, our application is running on our EC2, but how are we going to communicate with it if it's listening on port 8080, but we only exposed port 80?".

To answer this question, we need to understand what is [Nginx](https://nginx.org/).

Nginx is a lightweight, high-performance HTTP web server that can do many things (you can check them out in the link above). In short, we'll use it as a **reverse proxy**. This means Nginx will sit in front of our application, receive all incoming traffic on port 80, and forward it to our Spring Boot application running internally on port 8080.

Here's a simple diagram showing how this works:

<img src="/assets/reverse-proxy-explanation.png" width="150"/><br><br>

Additionally, we'll add:

- A `/health` route to check Nginx status
- A rate limit of 10 requests per second
- Some timeout and buffers to deal with our backend requests.

First, remove the default Nginx configuration:

```bash
sudo rm /etc/nginx/sites-enabled/default
```

Our nginx configuration file will look like this:

_Side note: Iâ€™ve added comments to make the file easier to understand._

```bash
sudo bash -c 'cat > /etc/nginx/sites-available/url' <<'EOF'
server {
  # Listen on port 80 as the default server.
  listen 80 default_server;
  # Accepts any host name.
  server_name _;

  # Health check route.
  location /health {
    default_type application/json;
    return 200 '{"status": "UP"}';
  }

  location / {
    # Uses the 'rl' zone that will be defined in nginx's http{} config.
    limit_req zone=rl burst=20 nodelay;
    # burst=20: allow up to 20 requests above the rate.
    # nodelay: do not delay requests within the burst and reject requests beyond the burst.

    # Forward to the backend on port 8080.
    proxy_pass http://127.0.0.1:8080;
    # Preserve original host header.
    proxy_set_header Host $host;
    # Preserve original client IP.
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    # Preserve original protocol.
    proxy_set_header X-Forwarded-Proto $scheme;

	# Upstream (backend) timeouts and buffering.
    # Maximum time to establish a TCP handshake.
    proxy_connect_timeout 5s;
    # Maximum time between two successive write operations to the upstream.
    proxy_send_timeout 60s;
    # Maximum time between two successive read operations from the upstream.
    proxy_read_timeout 60s;
  }
}
EOF
```

Before activating our configuration file, we need to define the rate limit zone and enable gzip compression.

To do this, add these lines to the http{} block in `/etc/nginx/nginx.conf` file:

```bash
# Creates the rate limit zone (10req/s)
limit_req_zone $binary_remote_addr zone=rl:10m rate=10r/s;

# Enables gzip compression.
gzip on;
gzip_min_length 1024;
gzip_types text/plain text/css application/json application/javascript application/xml image/svg+xml;
```

Let's activate our new configuration by creating a symbolic link in our `/etc/nginx/sites-enabled/` directory:

```bash
sudo ln -sf /etc/nginx/sites-available/url /etc/nginx/sites-enabled/url
```

Restart nginx to make changes:

```bash
sudo nginx -t && sudo systemctl reload nginx
```

You can check our reverse-proxy functioning by sending a request to the health check route:

```bash
curl -v http://your_ip/health
```

It should return `200`.

With that, we're done with our **initial** (!) EC2 setup!
